# -*- coding: utf-8 -*-
"""DataMining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18lXlX810aOAOAU6POot9l2F7StZcUlx6

#Import Libraries
"""

!nvidia-smi

pip install wordcloud

# pip install keybert

import nltk
nltk.download('omw-1.4')

import numpy as np
from keras.datasets import imdb
from matplotlib import pyplot as plt
import pandas as pd
import json
from keras.layers.embeddings import Embedding
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import nltk
import seaborn as sns
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
import pprint
import nltk
nltk.download('wordnet')
from keybert import KeyBERT
from collections import Counter
from wordcloud import WordCloud

"""#Data Loading"""

#Train
df = pd.read_csv("TUM.csv",  encoding = "ISO-8859-1")
df = df.dropna(axis=1,how='all')
df = df.dropna()
# print(df)
print(len(df))
df.to_csv('TUM_after_Preprocesing.csv', index=False)

"""#Preprocessing"""

columns = []
for i in range(len(df.columns)):
  columns.append(df.columns[i])
columns

print(columns.index("Objective"),
columns.index("Description"),
columns.index("Prerequisite"))

import re
words = {}
features = [columns.index("Objective"), columns.index("Description"),columns.index("Prerequisite"), columns.index("Required Skills")]
print(features)
#Train
for index, row in df.iterrows():
  rowFeatures=[]
  for ind in features:
    rowFeatures.append(re.split(r'\W+', row[ind]))
  rowFeatures = [j for sub in rowFeatures for j in sub]           # to 1d array
  # print(rowFeatures)

  # if index==3:
  #   print(rowFeatures)
    
  #Stemming:
  from nltk.stem import PorterStemmer
  porter = PorterStemmer()
  for idx,word in enumerate(rowFeatures):
    rowFeatures[idx] = porter.stem(word)
  # print(rowFeatures)

  # if index==3:
  #   print(rowFeatures)

  #Lemmatizing:
  from nltk.stem import WordNetLemmatizer
  lemmatizer = WordNetLemmatizer()
  for idx,word in enumerate(rowFeatures):
    rowFeatures[idx] = lemmatizer.lemmatize(word)
  # print(rowFeatures)


  # if index==3:
  #   print(rowFeatures)


  #Remove Stopwords:
  from nltk.corpus import stopwords
  stopWords = set(stopwords.words('english'))
  for idx,word in enumerate(rowFeatures):
    if word in stopWords:
      rowFeatures.remove(word)

    # if index==3:
    #   print(rowFeatures)
  words[index] = rowFeatures
    # print(names[index])

      
# print(labels)
print(len(words.values()))

print(words)

"""#Key-words Extraction Using  KeyBert"""

kw_model = KeyBERT()
arrOfKeywords=[]

#Takes 3 mintues.....
for key, el in words.items():
    if len(el)==0:
      continue
    wordsString = ' '.join(el)
    keywords = kw_model.extract_keywords(wordsString, keyphrase_ngram_range=(1, 1), stop_words=None)
    arrOfKeywords.append(keywords)

# arr = [j for sub in arr for j in sub]
# onlyWordsOfRow = list(map(list, zip(*row)))[0] 
print(arrOfKeywords)

s = ''
for i in range(len(arrOfKeywords)):
  for (word, score) in arrOfKeywords[i]:
    s += word + ' '

wordcloud = WordCloud(width = 800, height = 800, background_color ='white', min_font_size = 10).generate(s)
plt.figure(figsize=(10,10))
plt.axis("off")
plt.imshow(wordcloud)

""" Creating Assiciated Table


"""

arrOfKeywordsWithoutSimilarity=[]
relatedNumKeyBert=[]
for row in arrOfKeywords:
  onlyWordsOfRow = list(map(list, zip(*row)))[0]                      # separate score of each word
  onlyNumOfRow = list(map(list, zip(*row)))[1]                      # separate score of each word
  if len(onlyWordsOfRow)==1:
    continue                                                        # noise
  relatedNumKeyBert.append(onlyNumOfRow)
  arrOfKeywordsWithoutSimilarity.append(onlyWordsOfRow)
# allWordsOfKeyBert=list(map(list, zip(*arr)))[0]
# countWords = Counter(allWordsOfKeyBert).items()                        # occarance of words
# sortedCountWords = sorted(countWords, key=lambda tup: tup[1], reverse = True) # sort it by number of them
# sortedCountWords

# allWordsOfKeyBert
# arr
# arrOfKeywordsWithoutSimilarity[290:300]
print(np.array(arrOfKeywordsWithoutSimilarity).shape,np.array(relatedNumKeyBert).shape)
# print(arrOfKeywords[0], "\n was converted to: \n",arrOfKeywordsWithoutSimilarity[0])

from mlxtend.preprocessing import TransactionEncoder

te = TransactionEncoder()
te_ary = te.fit(arrOfKeywordsWithoutSimilarity).transform(arrOfKeywordsWithoutSimilarity)
associatedTable = pd.DataFrame(te_ary, columns=te.columns_)
df.to_csv('associatedTable.csv', index=False)
associatedTable

from mlxtend.frequent_patterns import apriori
lenOfAssociatedRelations=[]
for minSupport in range(10,100,10):
  ind = minSupport/100
  associatedRelations = apriori(associatedTable, min_support=ind , use_colnames=True)
  lenOfAssociatedRelations.append(len(associatedRelations))

plt.xlabel("min Support")
plt.ylabel("number of relations among words")
plt.plot(["0.1","0.2","0.3","0.4","0.5","0.6","0.7","0.8","0.9",], lenOfAssociatedRelations )
plt.show()
print(lenOfAssociatedRelations)

associatedRelations = apriori(associatedTable, min_support=0.1)
print("Recognized associated relations among row-indexes for min-support=0.1\n \n     Total rows of database=337\n\n" , associatedRelations)

"""#Third Phase

##Creating Embeddings
"""

!pip -q install sentence-transformers
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
model

finalWords  = []
courseTitles = []
for key in words.keys():
  finalWords.append(words[key])
  courseTitles.append(df['Course title'][key])
print(finalWords)
print(courseTitles)

embeddings = model.encode(finalWords, normalize_embeddings=True)
embeddings.shape

"""##Kmeans Clustring"""

elbow_dict ={}

from sklearn.cluster import KMeans
nClusters = 50
elbow_dict[nClusters] = []
kmeans = KMeans(n_clusters=nClusters)
kmeans.fit(embeddings)
kmeansPredictions = kmeans.predict(embeddings)
# kmeansPredictions

clusters_label = {}
clusters_data = {}
courseTitles = np.asarray(courseTitles)
for i in range(nClusters):
  clusters_label[i] = []
  cluster = np.where(kmeansPredictions==i)[0]
  clusters_label[i] = courseTitles[cluster]
  clusters_data[i] = embeddings[cluster]
  print(f"Size of Cluster {i} is {clusters_data[i].shape[0]}")
  # print(len(clusters_label[i]))
  # print(clusters_data[i].shape[0])
# clusters_label

"""##Clustring Evaluation"""

intraDistance = 0
for i in range(nClusters):
  data = clusters_data[i]
  for j in range(data.shape[0]):
    for k in range(j+1, data.shape[0]):
       intraDistance += np.sum(np.square(data[j] - data[k]))     
print(intraDistance)

interDistance = 0
for i in range(nClusters):
  for j in range(i+1, nClusters):
    dataCluster1 = clusters_data[i]
    dataCluster2 = clusters_data[j]
    for a in range(dataCluster1.shape[0]):
      for b in range(dataCluster2.shape[0]):
        interDistance += np.sum(np.square(dataCluster1[a] - dataCluster2[b]))   
print(interDistance)

elbow_dict[nClusters] = [intraDistance, interDistance]

"""##Elbow Plot"""

plt.figure(figsize = (10,8))

for key, (d1, d2) in elbow_dict.items():
  plt.scatter(key, d1, color = "b")
  plt.scatter(key, d2, color = "r")
  plt.scatter(key, (d1+d2)/2, color = "g")
plt.legend(["IntraDistance", "InterDistance", "Avg"])
plt.xlabel("Number of Clusters")
plt.ylabel("Distance")
  # print(key)

for i in range(nClusters):
  print(clusters_label[i])

"""##Classification"""

embeddings.shape

x_train_informatic = np.array(embeddings)
y_train_informatic = np.ones(embeddings.shape[0])
x_train_nonInformatic = np.array(embeddings)
y_train_nonInformatic = np.zeros(embeddings.shape[0])

data = np.vstack((x_train_informatic, x_train_nonInformatic))
label = np.hstack((y_train_informatic, y_train_nonInformatic))
print(data.shape)
print(label.shape)
data, label = shuffle(data, label)
# print(label)

X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.3, random_state=42)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""##KNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score
acc_knn = []

k_values = list(range(1, 11))
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    score = knn.score(X_test, y_test)
    acc_knn.append(score)
    print(f"Accuracy obtained using K = {k} is: {score}")

plt.figure(figsize = (10,8))
plt.plot(k_values, acc_knn)

"""##Perceptron"""

from sklearn.pipeline import make_pipeline
from sklearn.linear_model import Perceptron
from sklearn.preprocessing import StandardScaler

perceptron = make_pipeline(StandardScaler(), Perceptron(
    penalty='l1',
    eta0=0.3,
    random_state=2000
))
perceptron.fit(X_train, y_train)
acc_perceptron = perceptron.score(X_test, y_test)
print(acc_perceptron)

"""##MLP"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.neural_network import MLPClassifier
mlp = make_pipeline(MinMaxScaler((-1,1)), 
    MLPClassifier(
        activation='relu',
        hidden_layer_sizes=X_train.shape[1],
        max_iter=1000,
        random_state=2000
    )
)
mlp.fit(X_train, y_train)

acc_mlp = mlp.score(X_test, y_test)

"""##SVM"""

from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, MinMaxScaler
svm = make_pipeline(StandardScaler(), SVC(kernel="rbf", C=2.5, degree=4, coef0=0.01, random_state=2000, max_iter = 5000))
svm.fit(X_train, y_train)
svm_accuracies = svm.score(X_test, y_test)

"""##Rest of the Code

**extract key numbers**
"""

#   relatedNumKeyBert was created above

print("len:", len(relatedNumKeyBert))
relatedNumKeyBert[290:300]
np.array(relatedNumKeyBert).shape

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

K_CLUSTER_NUMBERS=2
vectorizer = TfidfVectorizer() 
kmeans = KMeans(n_clusters = K_CLUSTER_NUMBERS, init = 'k-means++').fit(relatedNumKeyBert)
# kmeans.labels_

clusters=np.zeros(K_CLUSTER_NUMBERS);cluster_1=[];cluster_2=[];cluster_3=[]
for wordlistIndx in range(0,len(arrOfKeywordsWithoutSimilarity)):
  cat = kmeans.labels_[wordlistIndx]
  if cat==0:
      cluster_0.append(arrOfKeywordsWithoutSimilarity[wordlistIndx])
  elif cat==1:
      cluster_1.append(arrOfKeywordsWithoutSimilarity[wordlistIndx])
  elif cat==2:
      cluster_2.append(arrOfKeywordsWithoutSimilarity[wordlistIndx])
  elif cat==3:    
      cluster_3.append(arrOfKeywordsWithoutSimilarity[wordlistIndx])

print("Clusters:")
print(cluster_0,'\n',cluster_1,'\n',cluster_2,'\n',cluster_3)

